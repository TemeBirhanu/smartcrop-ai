{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SmartCrop AI - Complete Training Pipeline\n",
        "\n",
        "\n",
        "**Steps:**\n",
        "1. Mount Google Drive\n",
        "2. Setup project directory\n",
        "3. Install dependencies\n",
        "4. Verify dataset structure\n",
        "5. Train models\n",
        "6. Run predictions\n",
        "7. Export models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Setup Project Directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set your Google Drive folder path\n",
        "PROJECT_DIR = '/content/drive/MyDrive/SmartCrop-AI'\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "# Extract project if needed (uncomment if you uploaded as zip)\n",
        "# !unzip -q smartcrop-ai-colab.zip -d .\n",
        "\n",
        "# Navigate to AI directory\n",
        "os.chdir('smartcrop-ai/ai')\n",
        "print(f\"Current directory: {os.getcwd()}\")\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA support\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install computer vision libraries\n",
        "!pip install -q opencv-python albumentations ultralytics segment-anything\n",
        "\n",
        "# Install model export tools\n",
        "!pip install -q onnx onnxruntime onnxscript tensorflow\n",
        "\n",
        "# Install data processing libraries\n",
        "!pip install -q pandas scikit-learn scikit-image\n",
        "\n",
        "# Install visualization libraries\n",
        "!pip install -q matplotlib seaborn grad-cam\n",
        "\n",
        "# Install utilities\n",
        "!pip install -q pyyaml omegaconf tqdm requests\n",
        "\n",
        "print(\"\\n‚úì All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation and GPU\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    !nvidia-smi\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected. Training will be slow on CPU.\")\n",
        "    print(\"Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Verify Dataset Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify dataset structure\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path('data/raw')\n",
        "print(\"Checking dataset structure...\")\n",
        "print(f\"Train folder exists: {(data_dir / 'train').exists()}\")\n",
        "print(f\"Val folder exists: {(data_dir / 'val').exists()}\")\n",
        "print(f\"Test folder exists: {(data_dir / 'test').exists()}\")\n",
        "\n",
        "# Count samples\n",
        "if (data_dir / 'train').exists():\n",
        "    train_crops = [d.name for d in (data_dir / 'train').iterdir() if d.is_dir()]\n",
        "    print(f\"\\n‚úì Found {len(train_crops)} crops in training set\")\n",
        "    print(f\"Sample crops: {train_crops[:5]}\")\n",
        "    \n",
        "    # Count images in first crop\n",
        "    if train_crops:\n",
        "        first_crop = data_dir / 'train' / train_crops[0]\n",
        "        diseases = [d.name for d in first_crop.iterdir() if d.is_dir()]\n",
        "        if diseases:\n",
        "            sample_count = len(list((first_crop / diseases[0]).glob('*.jpg'))) + \\\n",
        "                          len(list((first_crop / diseases[0]).glob('*.JPG')))\n",
        "            print(f\"Sample: {train_crops[0]}/{diseases[0]} has {sample_count} images\")\n",
        "\n",
        "print(\"\\n‚úì Dataset is ready for training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: (Optional) Reduce Dataset Size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Optional) Reduce dataset for faster training\n",
        "# Skip this cell if you want to use the full dataset\n",
        "# This keeps small classes intact and reduces large classes\n",
        "\n",
        "# Uncomment the line below to run reduction:\n",
        "# !python scripts/reduce_dataset.py\n",
        "\n",
        "# When prompted, type 'y' to proceed\n",
        "print(\"Skipping dataset reduction. Uncomment the line above to reduce dataset size.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is not needed since dataset is already organized\n",
        "# Dataset structure is already in data/raw/train/, data/raw/val/, data/raw/test/\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset is already organized - no need to run this\n",
        "# If you need to reorganize, uncomment below:\n",
        "# !python scripts/organize_datasets.py\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is not needed - reduction is in Step 5 above\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Train MobileNetV3 Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train MobileNetV3 (on-device model)\n",
        "# This will take 30-60 minutes depending on dataset size\n",
        "\n",
        "!python train.py --model mobilenet_v3 --data-dir data/raw --epochs 10 --batch-size 32 --lr 0.001\n",
        "\n",
        "print(\"\\n‚úì Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check training results\n",
        "!ls -lh outputs/models/checkpoints/\n",
        "!tail -50 outputs/logs/training.log\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify model file exists before exporting\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "model_path = Path('outputs/models/checkpoints/best_model.pth')\n",
        "if model_path.exists():\n",
        "    size_mb = model_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"‚úì Found model: {model_path}\")\n",
        "    print(f\"  Size: {size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Model not found at: {model_path}\")\n",
        "    print(\"Available files in checkpoints:\")\n",
        "    checkpoint_dir = Path('outputs/models/checkpoints')\n",
        "    if checkpoint_dir.exists():\n",
        "        for f in checkpoint_dir.glob('*.pth'):\n",
        "            print(f\"  - {f.name}\")\n",
        "    else:\n",
        "        print(\"  Checkpoints directory doesn't exist!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Export Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export MobileNetV3 to mobile formats\n",
        "# Note: Model is saved as 'best_model.pth' (not mobilenet_v3_best.pth)\n",
        "!python export_model.py --model mobilenet_v3 --checkpoint outputs/models/checkpoints/best_model.pth\n",
        "\n",
        "# Verify exports\n",
        "!ls -lh outputs/models/*.tflite 2>/dev/null || echo \"No TFLite files\"\n",
        "!ls -lh outputs/models/*.onnx 2>/dev/null || echo \"No ONNX files\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Run Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload a test image\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get uploaded filename\n",
        "import os\n",
        "image_file = list(uploaded.keys())[0]\n",
        "print(f\"Testing on: {image_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run prediction with heatmap\n",
        "# Note: Model is saved as 'best_model.pth' (not mobilenet_v3_best.pth)\n",
        "!python predict.py --image {image_file} --model outputs/models/checkpoints/best_model.pth --model-type mobilenet_v3 --heatmap --output outputs/result.jpg\n",
        "\n",
        "# Display result\n",
        "from IPython.display import Image, display\n",
        "display(Image('outputs/result.jpg'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Predict with Severity (Classification + YOLO + SAM)\n",
        "\n",
        "**This combines all models to calculate disease severity:**\n",
        "- Classification model ‚Üí Disease type\n",
        "- YOLOv8 ‚Üí Lesion count\n",
        "- SAM ‚Üí Affected area percentage\n",
        "- Combined ‚Üí Severity level (Low/Moderate/High/Critical)\n",
        "\n",
        "**Note:** Requires SAM checkpoint. See `SAM_SETUP_GUIDE.md` for download instructions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Download SAM checkpoint (if not already downloaded)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "sam_dir = Path('outputs/models/sam')\n",
        "sam_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "sam_checkpoint = sam_dir / 'sam_vit_b.pth'\n",
        "\n",
        "if not sam_checkpoint.exists():\n",
        "    print(\"üì• Downloading SAM checkpoint (375MB)...\")\n",
        "    print(\"   This may take a few minutes...\")\n",
        "    checkpoint_path = str(sam_checkpoint)\n",
        "    !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth -O {checkpoint_path}\n",
        "    print(f\"‚úì SAM checkpoint downloaded to {sam_checkpoint}\")\n",
        "else:\n",
        "    print(f\"‚úì SAM checkpoint already exists at {sam_checkpoint}\")\n",
        "\n",
        "# Verify checkpoint size\n",
        "if sam_checkpoint.exists():\n",
        "    size_mb = sam_checkpoint.stat().st_size / (1024 * 1024)\n",
        "    print(f\"  Checkpoint size: {size_mb:.1f} MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Complete Severity Pipeline (Classification + Trained YOLOv8 + SAM)\n",
        "# Uses the same image from Step 8, or upload a new one\n",
        "\n",
        "# Use image from previous prediction cell, or upload new one\n",
        "try:\n",
        "    image_file  # Use image from Step 8\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è  No image found. Uploading new image...\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    image_file = list(uploaded.keys())[0]\n",
        "\n",
        "# Run the complete severity pipeline\n",
        "# Using quotes around image_file to handle spaces/special characters\n",
        "# Model type is auto-detected (defaults to efficientnet_b3)\n",
        "# To specify model type: add --model-type mobilenet_v3 or --model-type efficientnet_b3\n",
        "!python predict_severity_complete.py --image \"{image_file}\" --output outputs/severity_result.jpg\n",
        "\n",
        "# Display result\n",
        "from IPython.display import Image, display\n",
        "from pathlib import Path\n",
        "\n",
        "result_path = Path('outputs/severity_result.jpg')\n",
        "if result_path.exists():\n",
        "    display(Image('outputs/severity_result.jpg'))\n",
        "    print(\"\\n‚úÖ Severity prediction completed!\")\n",
        "    print(\"\\nThe output above shows:\")\n",
        "    print(\"  - Disease type and confidence\")\n",
        "    print(\"  - Severity level (Low/Moderate/High/Critical)\")\n",
        "    print(\"  - Affected area percentage\")\n",
        "    print(\"  - Lesion count and density\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Result image not found. Check the output above for errors.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Use YOLOv8 for Object Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use pretrained YOLOv8 for object detection \n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "from src.models.yolo_detector import YOLODetector\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the same image you used for prediction (or upload a new one)\n",
        "# If you haven't uploaded an image yet, uncomment below:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# image_file = list(uploaded.keys())[0]\n",
        "\n",
        "# Or use the image from previous prediction\n",
        "try:\n",
        "    image_file  # Use image from previous cell\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è  No image found. Please run the prediction cell first or upload an image.\")\n",
        "    print(\"Uploading new image...\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    image_file = list(uploaded.keys())[0]\n",
        "\n",
        "print(f\"üì∏ Using image: {image_file}\")\n",
        "\n",
        "# Load image\n",
        "image = cv2.imread(image_file)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Initialize YOLOv8 detector (pretrained - auto-downloads on first use)\n",
        "print(\"ü§ñ Loading pretrained YOLOv8-nano...\")\n",
        "detector = YOLODetector(model_size=\"n\", pretrained=True)\n",
        "\n",
        "# Run detection\n",
        "print(\"üîç Running detection...\")\n",
        "results = detector.detect(image_rgb, conf_threshold=0.25)\n",
        "\n",
        "# Print results\n",
        "print(f\"\\n‚úÖ Detection Results:\")\n",
        "print(f\"   Found {results['count']} objects\")\n",
        "\n",
        "if results['count'] > 0:\n",
        "    print(f\"\\n   Detections:\")\n",
        "    for i, (box, score, cls) in enumerate(zip(results['boxes'], results['scores'], results['classes'])):\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        print(f\"   {i+1}. Object class {cls}: confidence {score:.2f}, box [{x1}, {y1}, {x2}, {y2}]\")\n",
        "else:\n",
        "    print(\"   No objects detected (try lowering confidence threshold)\")\n",
        "\n",
        "# Draw bounding boxes\n",
        "img_with_boxes = image.copy()\n",
        "for box, score, cls in zip(results['boxes'], results['scores'], results['classes']):\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    label = f\"Class {cls} ({score:.2f})\"\n",
        "    cv2.putText(img_with_boxes, label, (x1, y1 - 10),\n",
        "               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "# Save and display\n",
        "output_path = 'outputs/yolo_detection.jpg'\n",
        "cv2.imwrite(output_path, img_with_boxes)\n",
        "display(Image(output_path))\n",
        "\n",
        "print(\"\\nüìù Note: YOLOv8 detects COCO classes (person, car, dog, etc.)\")\n",
        "print(\"   For plant disease lesion detection, you'd need to train on annotated plant data.\")\n",
        "print(\"   See YOLO_TRAINING_GUIDE.md for details.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Save Models to Google Drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: (Optional) Train Additional Models\n",
        "\n",
        "**Note:** MobileNetV3 is sufficient for most use cases! Only train these if you need specific features.\n",
        "\n",
        "- **EfficientNet-B3**: For higher accuracy (server/cloud use)\n",
        "- **YOLOv8**: For lesion detection (requires annotated data)\n",
        "- **SAM**: No training needed (already pretrained)\n",
        "\n",
        "See `MODELS_GUIDE.md` for details on when to use each model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: Train YOLOv8 on PlantDoc Dataset\n",
        "\n",
        "**Train YOLOv8 to detect plant disease lesions:**\n",
        "- Uses PlantDoc dataset (already in `data/yolo/`)\n",
        "- Fine-tunes pretrained YOLOv8 on plant disease lesions\n",
        "- After training, YOLOv8 will detect actual plant disease lesions (not just general objects)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Manual Cleanup (if fix cell fails)\n",
        "\n",
        "If you get \"File exists\" errors, run this cleanup cell first:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Manual cleanup: Remove any existing valid symlinks/directories\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "yolo_dir = Path('data/yolo')\n",
        "valid_dir = yolo_dir / 'valid'\n",
        "test_dir = yolo_dir / 'test'\n",
        "\n",
        "print(\"Cleaning up any existing 'valid' directory/symlink...\")\n",
        "\n",
        "if valid_dir.exists():\n",
        "    try:\n",
        "        if valid_dir.is_symlink():\n",
        "            print(f\"  Removing symlink: {valid_dir}\")\n",
        "            valid_dir.unlink()\n",
        "        else:\n",
        "            print(f\"  Removing directory: {valid_dir}\")\n",
        "            shutil.rmtree(valid_dir)\n",
        "        print(f\"  ‚úì Cleaned up\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è  Error: {e}\")\n",
        "        # Force remove\n",
        "        try:\n",
        "            os.remove(valid_dir) if valid_dir.is_symlink() else shutil.rmtree(valid_dir, ignore_errors=True)\n",
        "            print(f\"  ‚úì Force removed\")\n",
        "        except:\n",
        "            print(f\"  ‚úó Could not remove - you may need to delete manually\")\n",
        "else:\n",
        "    print(f\"  ‚úì No 'valid' directory found - nothing to clean\")\n",
        "\n",
        "# Also check if test is a symlink pointing to valid (wrong direction)\n",
        "if test_dir.exists() and test_dir.is_symlink():\n",
        "    try:\n",
        "        target = test_dir.readlink()\n",
        "        if 'valid' in str(target):\n",
        "            print(f\"\\n  ‚ö†Ô∏è  Warning: 'test' is a symlink pointing to 'valid' (wrong direction)\")\n",
        "            print(f\"     This might cause issues. Consider removing it manually if needed.\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(f\"\\n‚úÖ Cleanup complete. Now run the fix cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Quick Fix: Create 'valid' directory (if training failed)\n",
        "\n",
        "If you got an error about missing 'valid/images', run this cell to fix it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick fix: Create 'valid' directory and fix data.yaml paths\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "yolo_dir = Path('data/yolo')\n",
        "val_dir = yolo_dir / 'val'\n",
        "valid_dir = yolo_dir / 'valid'\n",
        "test_dir = yolo_dir / 'test'\n",
        "yaml_file = yolo_dir / 'data.yaml'\n",
        "\n",
        "print(\"Checking dataset structure...\")\n",
        "print(f\"  train exists: {(yolo_dir / 'train').exists()}\")\n",
        "print(f\"  test exists: {test_dir.exists()}\")\n",
        "print(f\"  val exists: {val_dir.exists()}\")\n",
        "print(f\"  valid exists: {valid_dir.exists()}\")\n",
        "\n",
        "# Check if valid is a symlink or real directory\n",
        "valid_is_symlink = valid_dir.is_symlink() if valid_dir.exists() else False\n",
        "valid_has_images = (valid_dir / 'images').exists() if valid_dir.exists() else False\n",
        "\n",
        "# Check if test is a symlink (might be pointing to valid - wrong direction!)\n",
        "test_is_symlink = test_dir.is_symlink() if test_dir.exists() else False\n",
        "if test_is_symlink:\n",
        "    try:\n",
        "        test_target = test_dir.readlink()\n",
        "        print(f\"  ‚ö†Ô∏è  'test' is a symlink pointing to: {test_target}\")\n",
        "        if 'valid' in str(test_target):\n",
        "            print(f\"     This is backwards! Fixing...\")\n",
        "            # Remove the symlink and restore test as a real directory\n",
        "            test_backup = yolo_dir / 'test_backup'\n",
        "            if not test_backup.exists():\n",
        "                # Copy test contents before removing symlink\n",
        "                if (Path(test_target) / 'images').exists():\n",
        "                    shutil.copytree(test_target, test_backup)\n",
        "                    test_dir.unlink()\n",
        "                    shutil.move(str(test_backup), str(test_dir))\n",
        "                    print(f\"     ‚úì Fixed: restored 'test' as real directory\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "# Fix valid directory - use COPY instead of symlink (more reliable on Google Drive)\n",
        "if valid_dir.exists() and valid_has_images:\n",
        "    print(f\"\\n‚úì 'valid' directory already exists and has images\")\n",
        "    if valid_is_symlink:\n",
        "        try:\n",
        "            link_target = valid_dir.readlink()\n",
        "            print(f\"   (It's a symlink to: {link_target})\")\n",
        "        except:\n",
        "            pass\n",
        "else:\n",
        "    # ALWAYS remove existing valid directory/symlink (even if empty or broken)\n",
        "    if valid_dir.exists():\n",
        "        print(f\"\\nüîß Removing existing 'valid' directory/symlink...\")\n",
        "        try:\n",
        "            if valid_dir.is_symlink():\n",
        "                valid_dir.unlink()\n",
        "                print(f\"   ‚úì Removed symlink\")\n",
        "            elif valid_dir.is_dir():\n",
        "                shutil.rmtree(valid_dir)\n",
        "                print(f\"   ‚úì Removed directory\")\n",
        "            else:\n",
        "                valid_dir.unlink()\n",
        "                print(f\"   ‚úì Removed file\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è  Error removing: {e}, trying force remove...\")\n",
        "            # Force remove - try multiple methods\n",
        "            import stat\n",
        "            try:\n",
        "                # Make writable if needed\n",
        "                if valid_dir.is_dir():\n",
        "                    for root, dirs, files in os.walk(valid_dir):\n",
        "                        for d in dirs:\n",
        "                            os.chmod(os.path.join(root, d), stat.S_IWRITE)\n",
        "                        for f in files:\n",
        "                            os.chmod(os.path.join(root, f), stat.S_IWRITE)\n",
        "                # Remove\n",
        "                if valid_dir.is_symlink():\n",
        "                    os.remove(valid_dir)\n",
        "                else:\n",
        "                    shutil.rmtree(valid_dir, ignore_errors=True)\n",
        "                print(f\"   ‚úì Force removed\")\n",
        "            except Exception as e2:\n",
        "                print(f\"   ‚úó Could not remove: {e2}\")\n",
        "                print(f\"   Please manually delete: {valid_dir}\")\n",
        "                raise\n",
        "    \n",
        "    # Verify valid_dir is gone before copying\n",
        "    if valid_dir.exists():\n",
        "        print(f\"\\n‚ö†Ô∏è  Warning: 'valid' still exists after removal attempt\")\n",
        "        print(f\"   Trying one more time...\")\n",
        "        import time\n",
        "        time.sleep(0.5)  # Brief pause\n",
        "        try:\n",
        "            if valid_dir.is_symlink():\n",
        "                os.remove(valid_dir)\n",
        "            else:\n",
        "                shutil.rmtree(valid_dir, ignore_errors=True)\n",
        "            if not valid_dir.exists():\n",
        "                print(f\"   ‚úì Successfully removed\")\n",
        "            else:\n",
        "                raise Exception(f\"Could not remove {valid_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚úó Failed: {e}\")\n",
        "            print(f\"   Please run this command manually:\")\n",
        "            print(f\"   !rm -rf '{valid_dir}'\")\n",
        "            raise\n",
        "    \n",
        "    # Create valid from val or test - USE COPY (more reliable than symlink on Drive)\n",
        "    if val_dir.exists() and (val_dir / 'images').exists():\n",
        "        print(f\"\\nüîß Creating 'valid' directory from 'val' (copying)...\")\n",
        "        shutil.copytree(val_dir, valid_dir)\n",
        "        print(f\"   ‚úì Copied {val_dir} to {valid_dir}\")\n",
        "    elif test_dir.exists() and (test_dir / 'images').exists():\n",
        "        print(f\"\\nüîß No 'val' found. Using 'test' as 'valid' (copying)...\")\n",
        "        print(f\"   Note: This will copy test set to valid (YOLO needs validation set)\")\n",
        "        shutil.copytree(test_dir, valid_dir)\n",
        "        print(f\"   ‚úì Copied {test_dir} to {valid_dir}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è  Could not create 'valid' directory\")\n",
        "        print(f\"   Neither 'val' nor 'test' has images/ subdirectory\")\n",
        "\n",
        "# Fix data.yaml paths to be correct (YOLOv8 format: path + relative paths)\n",
        "if yaml_file.exists():\n",
        "    print(f\"\\nüìù Fixing data.yaml paths...\")\n",
        "    with open(yaml_file, 'r') as f:\n",
        "        yaml_data = yaml.safe_load(f)\n",
        "    \n",
        "    # Get absolute path to yolo directory\n",
        "    yolo_abs = yolo_dir.absolute()\n",
        "    \n",
        "    print(f\"   Current paths in data.yaml:\")\n",
        "    print(f\"     path: {yaml_data.get('path', 'NOT SET')}\")\n",
        "    print(f\"     train: {yaml_data.get('train', 'NOT SET')}\")\n",
        "    print(f\"     val: {yaml_data.get('val', 'NOT SET')}\")\n",
        "    \n",
        "    # YOLOv8 format: path is base directory, train/val/test are relative to path\n",
        "    # Format: path: /absolute/path/to/dataset\n",
        "    #         train: train/images  (relative to path)\n",
        "    #         val: valid/images    (relative to path)\n",
        "    yaml_data['path'] = str(yolo_abs)  # Absolute base path\n",
        "    yaml_data['train'] = 'train/images'  # Relative to path\n",
        "    yaml_data['val'] = 'valid/images'    # Relative to path (not val!)\n",
        "    if test_dir.exists():\n",
        "        yaml_data['test'] = 'test/images'  # Relative to path\n",
        "    \n",
        "    # Write updated yaml\n",
        "    with open(yaml_file, 'w') as f:\n",
        "        yaml.dump(yaml_data, f, default_flow_style=False, sort_keys=False)\n",
        "    \n",
        "    print(f\"\\n   ‚úì Updated data.yaml (YOLOv8 format):\")\n",
        "    print(f\"     path: {yaml_data['path']}\")\n",
        "    print(f\"     train: {yaml_data['train']} (relative to path)\")\n",
        "    print(f\"     val: {yaml_data['val']} (relative to path)\")\n",
        "    if 'test' in yaml_data:\n",
        "        print(f\"     test: {yaml_data['test']} (relative to path)\")\n",
        "    \n",
        "    # Verify paths exist (combine path + relative path, resolve symlinks)\n",
        "    print(f\"\\n   Verifying paths exist:\")\n",
        "    base_path = Path(yaml_data['path'])\n",
        "    for key in ['train', 'val', 'test']:\n",
        "        if key in yaml_data:\n",
        "            # Combine base path with relative path\n",
        "            full_path = base_path / yaml_data[key]\n",
        "            # Resolve symlinks\n",
        "            resolved_path = full_path.resolve()\n",
        "            exists = resolved_path.exists()\n",
        "            if exists:\n",
        "                count = len(list(resolved_path.glob('*.jpg'))) + len(list(resolved_path.glob('*.png')))\n",
        "                print(f\"     ‚úì {key}: {count} images\")\n",
        "                if full_path != resolved_path:\n",
        "                    print(f\"       (resolved from: {full_path} -> {resolved_path})\")\n",
        "            else:\n",
        "                print(f\"     ‚úó {key}: Path does not exist!\")\n",
        "                print(f\"       Looking for: {full_path}\")\n",
        "                print(f\"       Resolved to: {resolved_path}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Ready for training! Run the training cell now.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Verify and fix dataset structure\n",
        "import os\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "yolo_dir = Path('data/yolo')\n",
        "print(\"Checking YOLO dataset structure...\")\n",
        "\n",
        "# Check for common structures\n",
        "possible_paths = [\n",
        "    'data/yolo/data.yaml',\n",
        "    'data/yolo/train/data.yaml',\n",
        "    'data/yolo/plantdoc/data.yaml',\n",
        "    'data/yolo/PlantDoc-1/data.yaml'\n",
        "]\n",
        "\n",
        "data_yaml = None\n",
        "for path in possible_paths:\n",
        "    if Path(path).exists():\n",
        "        data_yaml = path\n",
        "        print(f\"‚úì Found data.yaml at: {path}\")\n",
        "        break\n",
        "\n",
        "if not data_yaml:\n",
        "    print(\"‚ö†Ô∏è  data.yaml not found. Checking directory structure...\")\n",
        "    print(f\"Contents of data/yolo/:\")\n",
        "    if yolo_dir.exists():\n",
        "        for item in sorted(yolo_dir.iterdir()):\n",
        "            print(f\"  - {item.name}\")\n",
        "    else:\n",
        "        print(\"  data/yolo/ directory doesn't exist!\")\n",
        "else:\n",
        "    # Check for images and labels\n",
        "    yaml_dir = Path(data_yaml).parent\n",
        "    print(f\"\\nDataset directory: {yaml_dir}\")\n",
        "    \n",
        "    # Check actual structure\n",
        "    actual_splits = {}\n",
        "    for split_name in ['train', 'val', 'valid', 'test']:\n",
        "        # Check different possible structures\n",
        "        possible_dirs = [\n",
        "            yaml_dir / split_name / 'images',\n",
        "            yaml_dir / 'images' / split_name,\n",
        "            yaml_dir / split_name,\n",
        "        ]\n",
        "        \n",
        "        for dir_path in possible_dirs:\n",
        "            if dir_path.exists():\n",
        "                img_count = len(list(dir_path.glob('*.jpg'))) + len(list(dir_path.glob('*.png')))\n",
        "                if img_count > 0:\n",
        "                    actual_splits[split_name] = {\n",
        "                        'images': dir_path,\n",
        "                        'count': img_count\n",
        "                    }\n",
        "                    print(f\"  ‚úì Found {split_name}: {img_count} images at {dir_path}\")\n",
        "                    break\n",
        "    \n",
        "    # Read and fix data.yaml if needed\n",
        "    if data_yaml:\n",
        "        print(f\"\\nüìù Reading data.yaml...\")\n",
        "        with open(data_yaml, 'r') as f:\n",
        "            yaml_data = yaml.safe_load(f)\n",
        "        \n",
        "        print(f\"  Current paths in data.yaml:\")\n",
        "        print(f\"    train: {yaml_data.get('train', 'NOT SET')}\")\n",
        "        print(f\"    val: {yaml_data.get('val', 'NOT SET')}\")\n",
        "        print(f\"    test: {yaml_data.get('test', 'NOT SET')}\")\n",
        "        \n",
        "        # Fix paths if needed - YOLOv8 expects 'valid' but many datasets use 'val'\n",
        "        needs_fix = False\n",
        "        yaml_path = Path(data_yaml).parent\n",
        "        \n",
        "        # Determine correct paths\n",
        "        if 'val' in actual_splits and 'valid' not in actual_splits:\n",
        "            # Dataset has 'val' but YOLO expects 'valid' - create symlink or fix yaml\n",
        "            val_path = actual_splits['val']['images'].parent\n",
        "            valid_path = yaml_path / 'valid'\n",
        "            \n",
        "            if not valid_path.exists() and val_path.exists():\n",
        "                print(f\"\\n  üîß Creating 'valid' symlink (YOLO expects 'valid' not 'val')...\")\n",
        "                import os\n",
        "                os.symlink(val_path, valid_path)\n",
        "                print(f\"    Created: {valid_path} -> {val_path}\")\n",
        "                needs_fix = True\n",
        "        \n",
        "        # Also fix yaml paths to be relative to yaml file location\n",
        "        if 'path' not in yaml_data or yaml_data['path'] != str(yaml_path):\n",
        "            yaml_data['path'] = str(yaml_path)\n",
        "            needs_fix = True\n",
        "        \n",
        "        # Update paths to be relative\n",
        "        for key in ['train', 'val', 'valid', 'test']:\n",
        "            if key in yaml_data:\n",
        "                path_val = yaml_data[key]\n",
        "                # If it's an absolute path or doesn't exist, fix it\n",
        "                if Path(path_val).is_absolute() or not (yaml_path / path_val).exists():\n",
        "                    # Try to find the correct relative path\n",
        "                    for split_name, split_info in actual_splits.items():\n",
        "                        if key in [split_name, 'valid' if split_name == 'val' else split_name]:\n",
        "                            rel_path = split_info['images'].relative_to(yaml_path)\n",
        "                            yaml_data[key] = str(rel_path.parent) if split_name == 'val' and key == 'valid' else str(rel_path)\n",
        "                            needs_fix = True\n",
        "                            break\n",
        "        \n",
        "        if needs_fix:\n",
        "            print(f\"\\n  üíæ Updating data.yaml...\")\n",
        "            with open(data_yaml, 'w') as f:\n",
        "                yaml.dump(yaml_data, f, default_flow_style=False)\n",
        "            print(f\"    ‚úì Updated data.yaml\")\n",
        "        \n",
        "        print(f\"\\n  Final data.yaml paths:\")\n",
        "        for key in ['train', 'val', 'valid', 'test']:\n",
        "            if key in yaml_data:\n",
        "                print(f\"    {key}: {yaml_data[key]}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Using data.yaml: {data_yaml}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Train YOLOv8 on PlantDoc dataset\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "from src.models.yolo_detector import YOLODetector\n",
        "\n",
        "# Find data.yaml (use the one found in previous cell)\n",
        "# If not found, try common locations\n",
        "data_yaml = None\n",
        "for path in ['data/yolo/data.yaml', 'data/yolo/train/data.yaml', 'data/yolo/plantdoc/data.yaml']:\n",
        "    if Path(path).exists():\n",
        "        data_yaml = path\n",
        "        break\n",
        "\n",
        "if not data_yaml:\n",
        "    print(\"‚ùå Error: data.yaml not found!\")\n",
        "    print(\"Please check that PlantDoc dataset is in data/yolo/\")\n",
        "else:\n",
        "    # Ensure 'valid' directory exists (YOLOv8 expects 'valid' not 'val')\n",
        "    yaml_dir = Path(data_yaml).parent\n",
        "    val_dir = yaml_dir / 'val'\n",
        "    valid_dir = yaml_dir / 'valid'\n",
        "    \n",
        "    if val_dir.exists() and not valid_dir.exists():\n",
        "        print(f\"üîß Creating 'valid' symlink (YOLO expects 'valid' not 'val')...\")\n",
        "        try:\n",
        "            os.symlink(val_dir, valid_dir)\n",
        "            print(f\"   ‚úì Created: {valid_dir} -> {val_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è  Could not create symlink: {e}\")\n",
        "            print(f\"   Trying to copy instead...\")\n",
        "            import shutil\n",
        "            shutil.copytree(val_dir, valid_dir)\n",
        "            print(f\"   ‚úì Copied {val_dir} to {valid_dir}\")\n",
        "    \n",
        "    print(f\"\\nüìä Training YOLOv8 on: {data_yaml}\")\n",
        "    print(\"   This will take 2-4 hours depending on dataset size...\")\n",
        "    print(\"   Training uses pretrained YOLOv8 weights (transfer learning)\")\n",
        "    \n",
        "    # Initialize YOLOv8 detector (pretrained)\n",
        "    detector = YOLODetector(model_size=\"n\", pretrained=True)\n",
        "    \n",
        "    # Train on PlantDoc dataset\n",
        "    # Adjust epochs/batch based on your dataset size\n",
        "    detector.train(\n",
        "        data_yaml=data_yaml,\n",
        "        epochs=50,      # 50 epochs is usually enough with pretrained weights\n",
        "        imgsz=416,      # Image size: 416=faster training, 640=better accuracy (slower)\n",
        "                        # For plant lesions, 416 is usually sufficient and trains 2-3x faster\n",
        "        batch=16        # Adjust based on GPU memory (16 for T4, 32 for A100)\n",
        "                        # With imgsz=416, you might be able to use batch=32 for faster training\n",
        "    )\n",
        "    \n",
        "    print(\"\\n‚úì YOLOv8 training completed!\")\n",
        "    print(\"  Model saved to: runs/detect/train/weights/best.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Test trained YOLOv8 model\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import Image, display\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project to path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "# Import YOLODetector\n",
        "from src.models.yolo_detector import YOLODetector\n",
        "\n",
        "# Use the same image from previous predictions, or upload new one\n",
        "try:\n",
        "    image_file  # Use image from Step 8\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è  No image found. Uploading new image...\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    image_file = list(uploaded.keys())[0]\n",
        "\n",
        "print(f\"üì∏ Testing on: {image_file}\")\n",
        "\n",
        "# Load image\n",
        "image = cv2.imread(image_file)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Load trained YOLOv8 model\n",
        "# YOLOv8 saves models in runs/detect/train*/weights/best.pt\n",
        "# Find the most recent training run\n",
        "runs_dir = Path('runs/detect')\n",
        "trained_model_path = None\n",
        "\n",
        "if runs_dir.exists():\n",
        "    # Find all train directories\n",
        "    train_dirs = sorted([d for d in runs_dir.iterdir() if d.is_dir() and d.name.startswith('train')])\n",
        "    if train_dirs:\n",
        "        # Try the most recent one\n",
        "        for train_dir in reversed(train_dirs):  # Start with most recent\n",
        "            model_path = train_dir / 'weights' / 'best.pt'\n",
        "            if model_path.exists():\n",
        "                trained_model_path = model_path\n",
        "                break\n",
        "\n",
        "if trained_model_path and Path(trained_model_path).exists():\n",
        "    print(f\"ü§ñ Loading trained YOLOv8 from: {trained_model_path}\")\n",
        "    detector = YOLODetector(model_size=\"n\", weights_path=str(trained_model_path))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Trained model not found. Using pretrained model instead.\")\n",
        "    print(f\"   Searched in: {runs_dir}\")\n",
        "    detector = YOLODetector(model_size=\"n\", pretrained=True)\n",
        "\n",
        "# Run detection\n",
        "print(\"üîç Running detection...\")\n",
        "results = detector.detect(image_rgb, conf_threshold=0.25)\n",
        "\n",
        "# Print results\n",
        "print(f\"\\n‚úÖ Detection Results:\")\n",
        "print(f\"   Found {results['count']} lesions\")\n",
        "\n",
        "if results['count'] > 0:\n",
        "    print(f\"\\n   Detections:\")\n",
        "    for i, (box, score, cls) in enumerate(zip(results['boxes'], results['scores'], results['classes'])):\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        print(f\"   {i+1}. Lesion class {cls}: confidence {score:.2f}, box [{x1}, {y1}, {x2}, {y2}]\")\n",
        "else:\n",
        "    print(\"   No lesions detected\")\n",
        "\n",
        "# Draw bounding boxes\n",
        "img_with_boxes = image.copy()\n",
        "for box, score, cls in zip(results['boxes'], results['scores'], results['classes']):\n",
        "    x1, y1, x2, y2 = map(int, box)\n",
        "    cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    label = f\"Lesion {cls} ({score:.2f})\"\n",
        "    cv2.putText(img_with_boxes, label, (x1, y1 - 10),\n",
        "               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "# Save and display\n",
        "output_path = 'outputs/yolo_trained_detection.jpg'\n",
        "cv2.imwrite(output_path, img_with_boxes)\n",
        "display(Image(output_path))\n",
        "\n",
        "print(\"\\nüìù Note: If you see 0 lesions, the model may need more training or different images.\")\n",
        "print(\"   Try lowering confidence threshold or training for more epochs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Optional) Train EfficientNet-B3 for higher accuracy\n",
        "# Only train if MobileNetV3 accuracy < 90% or you need server verification\n",
        "# This takes 6-8 hours\n",
        "\n",
        "# Uncomment to train:\n",
        "# !python train.py --model efficientnet_b3 --data-dir data/raw --epochs 5 --batch-size 32 --image-size 224 --lr 0.001\n",
        "\n",
        "print(\"Skipping EfficientNet-B3 training. MobileNetV3 is sufficient for most use cases.\")\n",
        "print(\"Uncomment the line above if you need higher accuracy.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 15: Growth Stage Classifier (Optional)\n",
        "\n",
        "**Classify crop growth stage (Seedling, Vegetative, Flowering, Maturity):**\n",
        "- Uses ResNet50 pretrained on ImageNet\n",
        "- Only trains the classification head (fast training)\n",
        "- Useful for growth recommendations and stage-specific advice\n",
        "\n",
        "**Note:** Requires growth stage labeled data organized as:\n",
        "```\n",
        "data/growth_stage/\n",
        "  train/\n",
        "    Seedling/\n",
        "    Vegetative/\n",
        "    Flowering/\n",
        "    Maturity/\n",
        "  val/\n",
        "    Seedling/\n",
        "    Vegetative/\n",
        "    Flowering/\n",
        "    Maturity/\n",
        "```\n",
        "\n",
        "**üì• Where to Get Growth Stage Datasets:**\n",
        "- See `GROWTH_STAGE_DATASET_GUIDE.md` for comprehensive dataset sources\n",
        "- **Quick options:**\n",
        "  - Create your own: Take photos of crops at different stages\n",
        "  - Use PlantNet/iNaturalist: Filter by growth stage metadata\n",
        "  - Search Kaggle: \"crop growth stage\" or \"phenology dataset\"\n",
        "  - Research datasets: Agricultural research institutions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Verify Growth Stage Dataset Structure\n",
        "from pathlib import Path\n",
        "\n",
        "growth_data_dir = Path('data/growth_stage')\n",
        "print(\"Checking growth stage dataset structure...\")\n",
        "\n",
        "if not growth_data_dir.exists():\n",
        "    print(f\"‚ö†Ô∏è  Growth stage data directory not found: {growth_data_dir}\")\n",
        "    print(\"\\nTo use growth stage classifier, organize your data as:\")\n",
        "    print(\"  data/growth_stage/\")\n",
        "    print(\"    train/\")\n",
        "    print(\"      Seedling/\")\n",
        "    print(\"      Vegetative/\")\n",
        "    print(\"      Flowering/\")\n",
        "    print(\"      Maturity/\")\n",
        "    print(\"    val/\")\n",
        "    print(\"      Seedling/\")\n",
        "    print(\"      Vegetative/\")\n",
        "    print(\"      Flowering/\")\n",
        "    print(\"      Maturity/\")\n",
        "else:\n",
        "    train_dir = growth_data_dir / 'train'\n",
        "    val_dir = growth_data_dir / 'val'\n",
        "    \n",
        "    print(f\"‚úì Found data directory: {growth_data_dir}\")\n",
        "    print(f\"  Train exists: {train_dir.exists()}\")\n",
        "    print(f\"  Val exists: {val_dir.exists()}\")\n",
        "    \n",
        "    if train_dir.exists():\n",
        "        stages = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])\n",
        "        print(f\"\\n  Found {len(stages)} growth stages: {', '.join(stages)}\")\n",
        "        \n",
        "        # Count images per stage\n",
        "        for stage in stages:\n",
        "            stage_dir = train_dir / stage\n",
        "            count = len(list(stage_dir.glob('*.jpg'))) + len(list(stage_dir.glob('*.JPG'))) + \\\n",
        "                   len(list(stage_dir.glob('*.png'))) + len(list(stage_dir.glob('*.PNG')))\n",
        "            print(f\"    {stage}: {count} images\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Dataset structure verified!\")\n",
        "    print(\"   Ready for training (run next cell)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Train Growth Stage Classifier\n",
        "# Only train if you have growth stage labeled data\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "growth_data_dir = Path('data/growth_stage')\n",
        "if not growth_data_dir.exists():\n",
        "    print(\"‚ö†Ô∏è  Growth stage data not found. Skipping training.\")\n",
        "    print(\"   See previous cell for dataset structure requirements.\")\n",
        "else:\n",
        "    print(\"üå± Training Growth Stage Classifier...\")\n",
        "    print(\"   This will take 30-60 minutes depending on dataset size\")\n",
        "    print(\"   Model: ResNet50 (pretrained on ImageNet)\")\n",
        "    print(\"   Only classification head is trained (backbone frozen)\\n\")\n",
        "    \n",
        "    !python train_growth_stage.py \\\n",
        "        --data-dir data/growth_stage \\\n",
        "        --epochs 20 \\\n",
        "        --batch-size 32 \\\n",
        "        --image-size 224 \\\n",
        "        --lr 0.001 \\\n",
        "        --output-dir outputs/models/growth_stage\n",
        "    \n",
        "    print(\"\\n‚úÖ Growth stage classifier training completed!\")\n",
        "    print(\"   Model saved to: outputs/models/growth_stage/best_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Predict Growth Stage\n",
        "# Use the same image from previous predictions, or upload a new one\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Use image from previous prediction cell, or upload new one\n",
        "try:\n",
        "    image_file  # Use image from Step 8\n",
        "except NameError:\n",
        "    print(\"‚ö†Ô∏è  No image found. Uploading new image...\")\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    image_file = list(uploaded.keys())[0]\n",
        "\n",
        "print(f\"üì∏ Predicting growth stage for: {image_file}\\n\")\n",
        "\n",
        "# Check if model exists\n",
        "model_path = Path('outputs/models/growth_stage/best_model.pth')\n",
        "if not model_path.exists():\n",
        "    print(\"‚ö†Ô∏è  Growth stage model not found!\")\n",
        "    print(f\"   Expected: {model_path}\")\n",
        "    print(\"   Please train the model first (Step 15, Cell 2)\")\n",
        "else:\n",
        "    # Run prediction\n",
        "    # Using quotes around image_file to handle spaces/special characters\n",
        "    !python predict_growth_stage.py --image \"{image_file}\" --model outputs/models/growth_stage/best_model.pth\n",
        "    \n",
        "    print(\"\\n‚úÖ Growth stage prediction completed!\")\n",
        "    print(\"\\nThe output above shows:\")\n",
        "    print(\"  - Predicted growth stage (Seedling/Vegetative/Flowering/Maturity)\")\n",
        "    print(\"  - Confidence score\")\n",
        "    print(\"  - Top 3 predictions with confidence scores\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory in Drive\n",
        "!mkdir -p /content/drive/MyDrive/SmartCrop-AI/models\n",
        "\n",
        "# Copy trained models\n",
        "!cp -r outputs/models/checkpoints/* /content/drive/MyDrive/SmartCrop-AI/models/\n",
        "\n",
        "# Copy exported models\n",
        "!cp outputs/models/*.tflite /content/drive/MyDrive/SmartCrop-AI/models/ 2>/dev/null || echo \"No TFLite files\"\n",
        "!cp outputs/models/*.onnx /content/drive/MyDrive/SmartCrop-AI/models/ 2>/dev/null || echo \"No ONNX files\"\n",
        "\n",
        "print(\"‚úì Models saved to Google Drive!\")\n",
        "print(\"Location: /content/drive/MyDrive/SmartCrop-AI/models/\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
