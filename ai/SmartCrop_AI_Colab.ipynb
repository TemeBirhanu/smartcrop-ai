{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SmartCrop AI - Complete Training Pipeline\n",
        "\n",
        "\n",
        "**Steps:**\n",
        "1. Mount Google Drive\n",
        "2. Setup project directory\n",
        "3. Install dependencies\n",
        "4. Verify dataset structure\n",
        "5. Train models\n",
        "6. Run predictions\n",
        "7. Export models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Setup Project Directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set your Google Drive folder path\n",
        "PROJECT_DIR = '/content/drive/MyDrive/SmartCrop-AI'\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "# Extract project if needed (uncomment if you uploaded as zip)\n",
        "# !unzip -q smartcrop-ai-colab.zip -d .\n",
        "\n",
        "# Navigate to AI directory\n",
        "os.chdir('smartcrop-ai/ai')\n",
        "print(f\"Current directory: {os.getcwd()}\")\n",
        "!ls -la\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA support\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install computer vision libraries\n",
        "!pip install -q opencv-python albumentations ultralytics segment-anything\n",
        "\n",
        "# Install model export tools\n",
        "!pip install -q onnx onnxruntime tensorflow\n",
        "\n",
        "# Install data processing libraries\n",
        "!pip install -q pandas scikit-learn scikit-image\n",
        "\n",
        "# Install visualization libraries\n",
        "!pip install -q matplotlib seaborn grad-cam\n",
        "\n",
        "# Install utilities\n",
        "!pip install -q pyyaml omegaconf tqdm requests\n",
        "\n",
        "print(\"\\n✓ All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify installation and GPU\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    !nvidia-smi\n",
        "else:\n",
        "    print(\"⚠️  No GPU detected. Training will be slow on CPU.\")\n",
        "    print(\"Go to Runtime → Change runtime type → GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Verify Dataset Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify dataset structure\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path('data/raw')\n",
        "print(\"Checking dataset structure...\")\n",
        "print(f\"Train folder exists: {(data_dir / 'train').exists()}\")\n",
        "print(f\"Val folder exists: {(data_dir / 'val').exists()}\")\n",
        "print(f\"Test folder exists: {(data_dir / 'test').exists()}\")\n",
        "\n",
        "# Count samples\n",
        "if (data_dir / 'train').exists():\n",
        "    train_crops = [d.name for d in (data_dir / 'train').iterdir() if d.is_dir()]\n",
        "    print(f\"\\n✓ Found {len(train_crops)} crops in training set\")\n",
        "    print(f\"Sample crops: {train_crops[:5]}\")\n",
        "    \n",
        "    # Count images in first crop\n",
        "    if train_crops:\n",
        "        first_crop = data_dir / 'train' / train_crops[0]\n",
        "        diseases = [d.name for d in first_crop.iterdir() if d.is_dir()]\n",
        "        if diseases:\n",
        "            sample_count = len(list((first_crop / diseases[0]).glob('*.jpg'))) + \\\n",
        "                          len(list((first_crop / diseases[0]).glob('*.JPG')))\n",
        "            print(f\"Sample: {train_crops[0]}/{diseases[0]} has {sample_count} images\")\n",
        "\n",
        "print(\"\\n✓ Dataset is ready for training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: (Optional) Reduce Dataset Size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Optional) Reduce dataset for faster training\n",
        "# Skip this cell if you want to use the full dataset\n",
        "# This keeps small classes intact and reduces large classes\n",
        "\n",
        "# Uncomment the line below to run reduction:\n",
        "# !python scripts/reduce_dataset.py\n",
        "\n",
        "# When prompted, type 'y' to proceed\n",
        "print(\"Skipping dataset reduction. Uncomment the line above to reduce dataset size.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is not needed since dataset is already organized\n",
        "# Dataset structure is already in data/raw/train/, data/raw/val/, data/raw/test/\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset is already organized - no need to run this\n",
        "# If you need to reorganize, uncomment below:\n",
        "# !python scripts/organize_datasets.py\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is not needed - reduction is in Step 5 above\n",
        "pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Train MobileNetV3 Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train MobileNetV3 (on-device model)\n",
        "# This will take 30-60 minutes depending on dataset size\n",
        "\n",
        "!python train.py --model mobilenet_v3 --data-dir data/raw --epochs 10 --batch-size 32 --lr 0.001\n",
        "\n",
        "print(\"\\n✓ Training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check training results\n",
        "!ls -lh outputs/models/checkpoints/\n",
        "!tail -50 outputs/logs/training.log\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Export Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export MobileNetV3 to mobile formats\n",
        "!python export_model.py --model mobilenet_v3 --checkpoint outputs/models/checkpoints/mobilenet_v3_best.pth\n",
        "\n",
        "# Verify exports\n",
        "!ls -lh outputs/models/*.tflite 2>/dev/null || echo \"No TFLite files\"\n",
        "!ls -lh outputs/models/*.onnx 2>/dev/null || echo \"No ONNX files\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Run Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload a test image\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get uploaded filename\n",
        "import os\n",
        "image_file = list(uploaded.keys())[0]\n",
        "print(f\"Testing on: {image_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run prediction with heatmap\n",
        "!python predict.py --image {image_file} --model outputs/models/checkpoints/mobilenet_v3_best.pth --model-type mobilenet_v3 --heatmap --output outputs/result.jpg\n",
        "\n",
        "# Display result\n",
        "from IPython.display import Image, display\n",
        "display(Image('outputs/result.jpg'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Save Models to Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory in Drive\n",
        "!mkdir -p /content/drive/MyDrive/SmartCrop-AI/models\n",
        "\n",
        "# Copy trained models\n",
        "!cp -r outputs/models/checkpoints/* /content/drive/MyDrive/SmartCrop-AI/models/\n",
        "\n",
        "# Copy exported models\n",
        "!cp outputs/models/*.tflite /content/drive/MyDrive/SmartCrop-AI/models/ 2>/dev/null || echo \"No TFLite files\"\n",
        "!cp outputs/models/*.onnx /content/drive/MyDrive/SmartCrop-AI/models/ 2>/dev/null || echo \"No ONNX files\"\n",
        "\n",
        "print(\"✓ Models saved to Google Drive!\")\n",
        "print(\"Location: /content/drive/MyDrive/SmartCrop-AI/models/\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
