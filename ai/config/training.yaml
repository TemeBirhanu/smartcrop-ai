# Training configuration

# General training settings
training:
  batch_size: 32
  num_epochs: 10
  num_workers: 4
  pin_memory: true
  
# Optimizer
optimizer:
  name: "adam"  # "adam", "sgd", "adamw"
  lr: 0.001
  weight_decay: 0.0001
  momentum: 0.9  # For SGD

# Learning rate scheduler
scheduler:
  name: "cosine"  # "cosine", "step", "reduce_on_plateau"
  step_size: 5  # For step scheduler
  gamma: 0.1  # For step scheduler
  T_max: 10  # For cosine scheduler
  min_lr: 0.00001

# Transfer learning
transfer_learning:
  freeze_backbone: true
  freeze_epochs: 5  # Freeze for first N epochs
  unfreeze_lr: 0.0001  # Lower LR when unfreezing

# Loss function
loss:
  name: "cross_entropy"  # "cross_entropy", "focal_loss"
  class_weights: null  # null or list of weights for imbalanced data

# Early stopping
early_stopping:
  enabled: true
  patience: 5
  min_delta: 0.001
  monitor: "val_loss"  # "val_loss" or "val_accuracy"

# Model checkpointing
checkpoint:
  save_best: true
  save_last: true
  save_every_n_epochs: 5
  monitor: "val_accuracy"
  mode: "max"  # "max" for accuracy, "min" for loss

